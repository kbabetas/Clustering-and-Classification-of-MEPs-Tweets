{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Classification of MEPs' Tweets\n",
    "\n",
    "You will investigate a dataset of tweets made by Members of the European Parliament. You will use data collected by Darko Cherepnalkoski, Andreas Karpf, Igor Mozetič, and Miha Grčar for their paper [Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166586) (while you are at it, why not read the paper as well?).\n",
    "\n",
    "This assignment is based on an original assignment by Ioannis Pavlopoulos (postdoc researcher) and Vasiliki Kougia (PhD candidate at AUEB).\n",
    "\n",
    "---\n",
    "\n",
    "> Panos Louridas, Associate Professor <br />\n",
    "> Department of Management Science and Technology <br />\n",
    "> Athens University of Economics and Business <br />\n",
    "> louridas@aueb.gr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "* Get the dataset from <https://www.clarin.si/repository/xmlui/handle/11356/1071>.\n",
    "\n",
    "* You will use the `retweets.csv` file.\n",
    "\n",
    "* Keep only the records for which the language of the original tweet is in English.\n",
    "\n",
    "* Get the text of the *original tweet* and add it to the dataset as an extra column. Use the Tweeter API to get the text (e.g., with Tweepy). In order not to run into rate limits you can ask for multiple tweets with one call.\n",
    "\n",
    "* Keep only the records for which you were able to download the tweet text.\n",
    "\n",
    "* Group the records by the European group of the MEP that posted the original tweet. If you see that there are groups with very few tweets (less than 50), drop them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "* Use k-means to cluster the tweets based on their text.\n",
    "\n",
    "* The tweet texts must be converted to a format suitable for k-means, bag of word matrices or tf-idf matrices.\n",
    "\n",
    "  * That means that you will need to use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) or a [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "  \n",
    "  * You may want to strip accents, and convert everything to lowercase. \n",
    "  \n",
    "  * Use `min_df=10` and `max_df=0.50`.\n",
    "  \n",
    "  * Remove all English stopwords.\n",
    "  \n",
    "  * You may experiment with additional ideas about how best to tokenize etc.\n",
    "\n",
    "* Use both the elbow method and the silhouette score to investigate the best number of clusters. Settle on the best number of clusters. \n",
    "\n",
    "* Visualize the clusters using Yellowbrick's [InterclusterDistance](https://www.scikit-yb.org/en/latest/api/cluster/icdm.html).\n",
    "  \n",
    "* Then, investigate the clusters by finding the most important features in each cluster. To do that, you will find the cluster center of each cluster, get the top $k$ dimensions (for $k$ say 10, 20, 30, ..., your choice) and print them out. The top $k$ dimensions are the $k$ dimensions with the biggest value in the cluster center. Do they make any sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "* Train at least two algorithms to learn to classify an unseen tweet. The target variable should be the political party of the original poster and the training features should be the original tweet's text.\n",
    "\n",
    "* You should split your data to training and testing datasets, try the different algorithms with cross validation on the training dataset, and find the best hyperparameters for the best algorithm. The best hyperparameters for the construction of the bag of words or tf-idf matrices for classification are not necessarily the same with those you used for k-means.\n",
    "\n",
    "* Report your scores; once you find the best algorithm and the best hyperparameters, report the score on the test data.\n",
    "\n",
    "* To gauge the efficacy of the algorithm, report also the results of a baseline classifier, using, for instance, scikit-learn's [`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor Code\n",
    "\n",
    "You understand that this is an individual assignment, and as such you must carry it out alone. You may seek help on the Internet, by Googling or searching in StackOverflow for general questions pertaining to the use of Python and pandas libraries and idioms. However, it is not right to ask direct questions that relate to the assignment and where people will actually solve your problem by answering them. You may discuss with your fellow students in order to better understand the questions, if they are not clear enough, but you should not ask them to share their answers with you, or to help you by giving specific advice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
